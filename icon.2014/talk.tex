\documentclass[landscape]{uedslides2C}
\usepackage{comment}
\usepackage{url}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{color}
\usepackage{colortbl}
\usepackage{epic,ecltree}
%\usepackage{bar}
\usepackage{eclbip}
\usepackage{fancybox}
%\usepackage{pause} % java -jar ~/Code/statmt/bin/pp4p.jar mtsummit09-talk.pdf mtsummit09-talk.view.pdf
\usepackage{pdfpages}
\usepackage{fancyvrb}
\usepackage[absolute]{textpos}
\renewcommand*\ttdefault{txtt} % 20% tighter than courier
%\usetikzlibrary{shapes}
%\usepackage{tikz-qtree}
\usepackage{natbib}
%\usepackage[english,vietnam]{babel}
%\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{rotating}
\usepackage[absolute]{textpos}

\usepackage{tikz}
\usepackage{tikz-qtree}
\usetikzlibrary{arrows,shapes,positioning}
\tikzstyle{textbase} = [text height=1.5ex,text depth=.25ex]
\tikzstyle{pipestep}=[draw,rounded corners,minimum width=3.4cm,textbase]%
\tikzstyle{model}=[pipestep,fill=blue!20]
\tikzstyle{input}=[pipestep,fill=black!50!white,text=white]
\tikzstyle{processing}=[pipestep,fill=green!20]
\tikzstyle{arr}=[->,arrows={-angle 90},line width=4pt,blue!40!black]
% For every picture that defines or uses external nodes, you'll have to
% apply the 'remember picture' style. To avoid some typing, we'll apply
% the style to all pictures.
\tikzstyle{every picture}+=[remember picture]
% By default all math in TikZ nodes are set in inline mode. Change this to
% displaystyle so that we don't get small fractions.
\everymath{\displaystyle}

\definecolor{lightblue}{rgb}{.8,.8,1}
\definecolor{mediumlightblue}{rgb}{.5,.5,1}
\definecolor{lightyellow}{rgb}{1,1,.5}
\definecolor{lightorange}{rgb}{1,.9,.7}
\definecolor{darkorange}{rgb}{1,.75,.2}
\definecolor{verydarkorange}{rgb}{.5,.3,0}
\definecolor{darkblue}{rgb}{0,0,0.8}
\definecolor{verydarkgreen}{rgb}{0,0.4,0}
\definecolor{darkgreen}{rgb}{0,0.8,0}
\definecolor{lightgreen}{rgb}{.8,1,.8}
\definecolor{lightred}{rgb}{1,.8,.8}
\definecolor{gray}{rgb}{0.9,0.9,0.9}
\definecolor{darkgrey}{rgb}{0.5,0.5,0.5}
\definecolor{verydarkgrey}{rgb}{0.3,0.3,0.3}
\definecolor{purple}{rgb}{0.6,0,0.6}
\definecolor{red}{rgb}{1,0,0}
\definecolor{orange}{rgb}{.8,.6,0}
\definecolor{cyan}{rgb}{0,.6,.6}

\newcommand{\newconcept}[1]{\textcolor{blue}{\bf #1}}
\newcommand{\example}[1]{\textcolor{darkblue}{\rm #1}}
\newcommand{\important}[1]{\textcolor{darkblue}{\em #1}}
\newcommand{\concept}[1]{\textcolor{darkblue}{\em #1}}
\newcommand{\maths}[1]{\textcolor{purple}{#1}}
\newcommand{\reference}[1]{\vspace{-2mm}\begin{flushright}\textcolor{purple}{\tiny [from #1]}\end{flushright}\vspace{-7mm}}

\newcommand{\highlightbox}[6]{\begin{textblock}{#3}(#1,#2) \colorbox{#4}{\textcolor{#5}{\begin{minipage}{#3in} #6 \end{minipage} }} \end{textblock}}
\newcommand{\backgroundbox}[5]{\highlightbox{#1}{#2}{#3}{#5}{black}{\vspace{#4in}\hspace{#3in}}}
\newcommand{\currenttopic}[1]{\colorbox{lightyellow}{\textcolor{black}{\bf #1}}}
\newcommand{\littlecode}[1]{\colorbox{gray}{\textcolor{black}{\small \tt #1}}}

\newcommand{\highlight}[1]{\colorbox{lightyellow}{#1}}
\newcommand{\highlightOrange}[1]{\colorbox{lightorange}{#1}}
\newcommand{\highlightGreen}[1]{\colorbox{lightgreen}{#1}}
\newcommand{\highlightBlue}[1]{\colorbox{lightblue}{#1}}

\bibliography{mt,more}

\begin{document}
\title[Machine Translation with Open Source Software]{{\sc \huge Moses}\\[3mm] Machine Translation with Open Source Software}
\author[Hoang, Huck and Koehn]{Hieu Hoang and Matthias Huck}
\date{\vspace{-5mm}December 2014}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction

\slide{Outline}
\vspace{10mm}

\begin{description}
\item[\small 09:30-10:15 $\;\;$] {\bf Introduction} % TODO: should check whether this schedule is realistic
\item[\small 10:15-11:00 $\;\;$] {\bf Hands-on Session} --- you will need a laptop
\item[\small 11:00-11:30 $\;\;$] Break
\item[\small 11:30-12:30 $\;\;$] {\bf Advanced Topics}
\end{description}
\vfill

Slides downloadable from \\ \\
\littlecode{\normalsize http://www.statmt.org/moses/icon.2014.pdf} % TODO: upload slides
\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Basic Idea}
\vspace{15mm}
\begin{center}
\includegraphics[scale=1.8]{basics.pdf}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Statistical Machine Translation History}
\vspace{10mm}
\begin{description}
\item[around 1990] $\;$\\[2mm] Pioneering work at IBM, inspired by success in speech recognition
\item[1990s] $\;$\\[2mm] Dominance of IBM's word-based models, support technologies
\item[early 2000s] $\;$\\[2mm] Phrase-based models
\item[late 2000s] $\;$\\[2mm] Tree-based models
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Moses History}
\vspace{10mm}
\begin{description} \itemsep -0.2mm
\item[2002] $\;$ Pharaoh decoder, precursor to Moses (phrase-based models)
\item[2005] $\;$ Moses started by Hieu Hoang and Philipp Koehn (factored models)
\item[2006] $\;$ JHU workshop extends Moses significantly
\item[2006-2012] $\;$ Funding by EU projects EuroMatrix, EuroMatrixPlus
\item[2009] $\;$ Tree-based models implemented in Moses
\item[2012-2015] $\;$ MosesCore project. Full-time staff to maintain and enhance Moses
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Moses in Academia}
\vspace{10mm}
\begin{itemize}
\item Built by academics, for academics
\item Reference implementation of state of the art
\begin{itemize}
\item researchers develop new methods on top of Moses
\item developers re-implement published methods
\item used by other researchers as black box
\end{itemize}
\item Baseline to beat
\begin{itemize}
\item researchers compare their method against Moses
\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Developer Community}
\begin{itemize} \itemsep -1mm
\item Main development at University of Edinburgh, but also: \vspace{-3mm}
\begin{itemize}
\item Fondazione Bruno Kessler (Italy)
\item Charles University (Czech Republic)
\item DFKI (Germany)
\item RWTH Aachen (Germany)
\item others \ldots
\end{itemize}
\item Code shared on github.com
\item Main forum: Moses support mailing list
\item Main event: Machine Translation Marathon\vspace{-3mm}
\begin{itemize}
\item annual open source convention
\item presentation of new open source tools
\item hands-on work on new open source projects
\item summer school for statistical machine translation
\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Open Source Components}
\vspace{10mm}
\begin{itemize}
\item Moses distribution uses external open source tools
\begin{itemize}
\item word alignment: {\sc giza++}, {\sc mgiza}, {\sc BerkeleyAligner}, {\sc FastAlign}
\item language model: {\sc srilm}, {\sc irstlm}, {\sc randlm}, {\sc kenlm}
\item scoring: {\sc bleu}, {\sc ter}, {\sc meteor}
\end{itemize}
\item Other useful tools
\begin{itemize}
\item sentence aligner
\item syntactic parsers
\item part-of-speech taggers
\item morphological analyzers
\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Other Open Source MT Systems}
\begin{itemize} \itemsep -3mm
\item {\bf Joshua} --- Johns Hopkins University\\
{\small \tt http://joshua.sourceforge.net/}
\item {\bf CDec} --- University of Maryland\\
{\small \tt http://cdec-decoder.org/}
\item {\bf Jane} --- RWTH Aachen\\
{\small \tt http://www.hltpr.rwth-aachen.de/jane/}
\item {\bf Phrasal} --- Stanford University\\
{\small \tt http://nlp.stanford.edu/phrasal/}
\item Very similar technology \vspace{-2mm}
\begin{itemize}
\item Joshua and Phrasal implemented in Java, others in C++
\item Joshua supports only tree-based models
\item Phrasal supports only phrase-based models
\end{itemize}
\item Open sourcing tools increasing trend in NLP research
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Moses in Industry}
\vspace{20mm}
\begin{itemize}
\item Distributed with LGPL --- free to use
\item Competitive with commercial SMT solutions\\ (Google, Microsoft, SDL Language Weaver, \ldots)
\item But:
\begin{itemize}
\item not easy to use
\item requires significant expertise for optimal performance
\item integration into existing workflow not straight-forward
\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Case Studies}
\vspace{5mm}
\begin{description} \itemsep -1mm
\item[European Commission] ---\\  uses Moses in-house to aid human translators
\item[Autodesk] --- \\ showed productivity increases in translating manuals when post-editing output from a custom-build Moses system
\item[Systran] --- \\ developed statistical post-editing using Moses
\item[Asia Online] --- \\ offers translation technology and services based on Moses
\item[Many others] \ldots \\ World Trade Organisation, Adobe, Symantec, WIPO, Sybase, Safaba, Bloomberg, Pangeanic, KatanMT, Capita, \ldots

\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Phrase-based Translation}
\begin{center} \vspace{15mm}
\includegraphics[scale=1.4]{phrase-model-alignment.pdf}
\end{center}\vspace{5mm}
\begin{itemize} \itemsep -1mm
\item Foreign input is segmented in phrases 
\item Each phrase is translated into English
\item Phrases are reordered
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Phrase Translation Options}
\vspace{-5mm}
\begin{center} 
\includegraphics[scale=1.5]{translation-options.pdf}
\end{center}\vspace{-10mm}
\begin{itemize}
\item Many translation options to choose from
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Phrase Translation Options}
\vspace{-5mm}
\begin{center} 
\includegraphics[scale=1.5]{translation-options-correct.pdf}
\end{center}  \vspace{-12mm}
\begin{itemize}
\item The machine translation decoder does not know the right answer\vspace{-4mm}
\begin{itemize}
\item picking the right translation options
\item arranging them in the right order \vspace{-6mm}
\end{itemize}
\item[$\rightarrow$] Search problem solved by beam search
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Decoding: Precompute Translation Options}
\begin{center}
\includegraphics[scale=1.3]{decoding-step1.pdf}\\[69mm]
consult phrase translation table for all input phrases
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Decoding: Start with Initial Hypothesis}
\begin{center} 
\includegraphics[scale=1.3]{decoding-step2.pdf}\\[22mm]
initial hypothesis: no input words covered, no output produced
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Decoding: Hypothesis Expansion}
\begin{center}
\includegraphics[scale=1.3]{decoding-step3.pdf}\\[22mm]
pick any translation option, create new hypothesis
\end{center} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Decoding: Hypothesis Expansion}
\begin{center} 
\includegraphics[scale=1.3]{decoding-step4.pdf}\\[5mm]
create hypotheses for all other translation options
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Decoding: Hypothesis Expansion}
\begin{center} 
\includegraphics[scale=1.3]{decoding-step5.pdf}\\
also create hypotheses from created partial hypothesis
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Decoding: Find Best Path}
\begin{center}
\includegraphics[scale=1.3]{decoding-step6.pdf}\\[1mm]
backtrack from highest scoring complete hypothesis
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\slide{Computational Complexity}
%\begin{itemize}\vspace{35mm} \itemsep 5mm
%\item The suggested process creates exponential number of hypothesis 
%\item Reduction of search space: pruning
%\item[$\rightarrow$] Decoder may not find the model-best translation
%\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Factored Translation}
\begin{itemize}
\item Factored represention of words 
\begin{center} \vspace{-8mm}
\includegraphics[width=155mm]{factors.pdf}
\end{center} \vspace{-22mm}
\item Goals  \vspace{-3mm}
\begin{itemize}
\item generalization, e.g. by translating lemmas, not surface forms
\item richer model, e.g. using syntax for reordering, language modeling
\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Factored Model}
\begin{center}
Example:\\
\includegraphics[scale=1.6]{factored-morphgen.pdf}\\
Decomposing the translation step\\
Translating lemma and morphological information more robust
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Syntax-based Translation}
\vspace{5mm}
{\small
\begin{center}
\begin{tabular}{cc}
\textcolor{black}{\bf String-to-String} & \textcolor{black}{\bf String-to-Tree} \\
\example{John misses Mary}
& \example{John misses Mary}\\
$\Rightarrow$  Marie manque \`a Jean
& $\Rightarrow$ \hspace{-2cm} \Tree [.S [.NP \example{Marie} ] [.VP [.V \example{manque} ] [.PP [.P \example{\`a} ] [.NP \example{Jean} ]  ]  ]  ]\\[5mm]

\textcolor{black}{\bf Tree-to-String} & \textcolor{black}{\bf Tree-to-Tree} \\
\Tree [.S [.NP \example{John} ] [.VP [.V \example{misses} ] [.NP \example{Mary} ]  ]  ] 
&  \Tree [.S [.NP \example{John} ] [.VP [.V \example{misses} ] [.NP \example{Mary} ]  ]  ] \hspace{-1cm} $\Rightarrow$ \hspace{-2cm}\Tree [.S [.NP \example{Marie} ] [.VP [.V \example{manque} ] [.PP [.P \example{\`a} ] [.NP \example{Jean} ]  ]  ]  ] \\[-1cm]
$\Rightarrow$ \example{Marie manque \`a Jean}  	
& 
\end{tabular}
\end{center}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Syntax-based Decoding}
\vspace{-31mm}
\begin{center}
\includegraphics[scale=1.15]{chart-parsing0.pdf}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Syntax-based Decoding}
\vspace{-31mm}
\begin{center}
\includegraphics[scale=1.15]{chart-parsing1.pdf}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Syntax-based Decoding}
\vspace{-31mm}
\begin{center}
\includegraphics[scale=1.15]{chart-parsing2.pdf}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Syntax-based Decoding}
\vspace{-31mm}
\begin{center}
\includegraphics[scale=1.15]{chart-parsing3.pdf}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Syntax-based Decoding}
\vspace{-31mm}
\begin{center}
\includegraphics[scale=1.15]{chart-parsing4.pdf}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Syntax-based Decoding}
\vspace{-31mm}
\begin{center}
\includegraphics[scale=1.15]{chart-parsing5.pdf}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Syntax-based Decoding}
\vspace{-31mm}
\begin{center}
\includegraphics[scale=1.15]{chart-parsing.pdf}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\slide{Advanced Features}
%\small
%\vspace{-5mm}
%\textcolor{darkgrey}{
%\begin{itemize} \itemsep -1mm
%%\item \currenttopic{How do I get started?}
%%\item {Experiment Management System}
%\item {Faster Training}
%\item {Faster Decoding}
%\item {Moses Server}
%\item {Data and domain adaptation}
%\item {Instructions to decoder}
%\item {Input formats}
%\item {Output formats}
%\item {Translation models}
%\item {Incremental Training}
%\end{itemize}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{How do I get started?}
\normalsize
\begin{itemize} \itemsep -1mm

\item{Collect your data}
  \begin{itemize}
  \item Parallel data 
        \begin{itemize}
        \item Freely available data, e.g.\ Europarl, MultiUN, WIT3, OPUS, \ldots
        \item TAUS, Linguistic Data Consortium (LDC), \ldots
        \end{itemize}
  \item Monolingual data
        \begin{itemize}
        \item CommonCrawl, WMT News Crawl corpus, \ldots
        \end{itemize}
%  \item Translation memories
  \end{itemize}
\item{Set up Moses}
  \begin{itemize}
  \item Download source code for Moses, GIZA++, MGIZA
  \item Compile, install
  \item Or use precompiled Moses packages (Windows, Linux, Mac\,OS\,X)
%        \begin{itemize}
%        \item Installer for Windows with GUI
%        \item Debian Linux RPMs
%        \item Binaries for Cygwin/Linux/Mac\,OS\,X
%        \end{itemize}
  \item More info: \url{http://www.statmt.org/moses/}
  \end{itemize}
\item{Train system}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Data-driven MT}
\vfill

\begin{center}\begin{tikzpicture}
\node[processing, text width=12em, align=center](ptraining){Training};
\node[input, text width=12em, align=center, above=1.5cm of ptraining](train){Parallel Training Data};
\node[model, text width=12em, align=center, below=1cm of ptraining](tm){Translation Model};
\draw[arr] (train) -- (ptraining);
\draw[arr] (ptraining) -- (tm);

\node[input, text width=12em, align=center, right=2cm of train](monotrain){Monolingual Training Data};
\node[model, text width=12em, align=center, right=2cm of tm] (lm) {Language Model};
\node[processing, text width=12em, align=center, right=2cm of ptraining] (lmtrain) {LM Estimation};
\draw[arr] (monotrain) -- (lmtrain);
\draw[arr] (lmtrain) -- (lm);

\node[processing, text width=12em, align=center, below right=2.5cm  and 1cm of tm.center] (tuning) {Tuning};
\node[input, text width=8em, align=center, right=2cm of tuning](dev){Development Set};
\node[model, text width=12em, align=center, below=1.5cm of tuning] (system) {SMT System};
\draw[arr, shorten >=4mm ] (lm) -- (tuning);
\draw[arr, shorten >=4mm] (tm) -- (tuning);
\draw[arr] (dev) -- (tuning);
\draw[arr] (tuning) -- (system);

\end{tikzpicture}\end{center}

\vfill

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Moses Pipeline}
{\small

\begin{center}
\textcolor{red}{Execute a lot of scripts} \\
\begin{SaveVerbatim}{myverb}
tokenize < corpus.en > corpus.en.tok
lowercase < corpus.en.tok > corpus.en.lc
...
mert.perl ....
moses ...
mteval-v13.pl ...
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}

\vspace{10mm}
\textcolor{red}{Change a part of the process, execute everything again} \\
\begin{SaveVerbatim}{myverb}
tokenize < corpus.en > corpus.en.tok
lowercase < corpus.en.tok > corpus.en.lc
...
mert.perl ....
moses ...
mteval-v13.pl ...
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\end{center}
\normalsize



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Phrase-based Model Training with Moses}
\vspace{20mm}
\begin{itemize}
\item Command line
\begin{center}
\begin{SaveVerbatim}{myverb}
train-model.perl ...
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\end{center}

\item Example phrase from model
\begin{center}
\footnotesize
\begin{SaveVerbatim}{myverb}
BŸndnisse ||| alliances ||| 1 1 1 1 2.718 ||| ||| 1 1
General Musharraf betrat am ||| general Musharraf appeared on ||| 1 1 1 1 2.718 ||| ||| 1 1

\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\end{center}
\end{itemize}

% phrase based
%  /Users/hieuhoang/workspace/sourceforge/trunk/scripts/training/train-model.perl -first-step 5 -last-step 6 --corpus corpus.1.0-0 -f de -e en -alignment-file /Users/hieuhoang/workspace/data/syntax-de-en-nc/10/aligned.1 -alignment  grow-diag-final-and -lexical-file lex 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Phrase-based Decoding with Moses}

\begin{itemize}
\item Command line   
\vspace{-4mm}
\begin{center}
\begin{SaveVerbatim}{myverb}
moses -f moses.ini -i in.txt > out.txt
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\end{center}
%\item Output
%\vspace{-4mm}
%\begin{center}
%\begin{SaveVerbatim}{myverb}
%there are various different opinions .
%\end{SaveVerbatim}
%\colorbox{gray}{\BUseVerbatim{myverb}}
%\end{center}
\item Advantages
\vspace{-1mm}
\begin{itemize}
\item fast --- under half a second per sentence for fast configuration
\item low memory requirements --- $\sim$200MB RAM for lowest configuration
%	\begin{itemize}
%	\item 200-300MB for lowest configuration
%	\item suitable for netbooks and mobile devices
%	\end{itemize}
\item state-of-the-art translation quality on most tasks,\\ especially for related language pairs
\item robust --- does not rely on any syntactic annotation
\end{itemize}
\item Disadvantages
\vspace{-1mm}
\begin{itemize}
\item poor modeling of linguistic knowledge and of long-distance dependencies
\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Hierarchical Model Training with Moses}
%\vspace{5mm}
\begin{itemize}
\item Hierarchical model:\\ formally syntax-based, without linguistic annotation (string-to-string)
%\vspace{-5mm}
\item Command line
\begin{center}
\begin{SaveVerbatim}{myverb} 
train-model.perl -hierarchical ...
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\end{center}
%\vspace{-5mm}
\item Example rule from model
\begin{center}
\vspace{-15mm}
\littlecode{\tiny B\"undnisse [X][X] Kr\"aften [X] ||| alliances [X][X] forces [X] ||| 1 1 1 1 2.718 ||| 1-1 ||| 0.0526316 0.0526316}
\end{center}
\item Visualization of rule
\vspace{-10mm}
\begin{center}
\tikzset{level distance=72pt}
\Tree [.X [. B\"undnisse ]  [.X$_1$ ] [. Kr\"aften ] ] $\Rightarrow$ \Tree [.X [. alliances ]  [.X$_1$ ] [. forces ] ]
\end{center}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Hierarchical Decoding with Moses}

\begin{itemize}
\item Command line   
\vspace{-4mm}
\begin{center}
\begin{SaveVerbatim}{myverb}
moses_chart -f moses.ini -i in.txt > out.txt
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\end{center}

%  /Users/hieuhoang/workspace/sourceforge/trunk/scripts/training/train-model.perl -first-step 5 -last-step 6 --corpus corpus.1.0-0 -f de -e en -alignment-file /Users/hieuhoang/workspace/data/syntax-de-en-nc/10/aligned.1 -alignment  grow-diag-final-and -lexical-file lex  -hierarchical

\item Advantages
\begin{itemize}
\item able to model non-contiguities like \emph{ne \ldots pas $\rightarrow$ not}
\item better at medium-range reordering
\item outperforms phrase-based systems when translating between widely different languages, e.g. Chinese-English
\end{itemize}

\item Disadvantages
\begin{itemize}
\item more disk usage --- translation model $\times$10 larger than phrase-based
\item slower --- 0.5 - 2 sec/sent.\ for fastest configuration
\item higher memory requirements --- more than 1GB RAM
\end{itemize}

\end{itemize}


%Advantages
%   - can outperform phrase-based models when translating between widely different languages
%	- Chinese-English
%         - only consistent perforance with hierarchical model 
%   - better at medium range re-ordering
%        
%   - theoretical
%       - language is recursive
%       - embed clause within clause
%       - e.g. 
%        This is the house that Jack built.
%
%        This is the malt that lay in the house that Jack built.
%
%        This is the mouse that ate the malt that lay in the house that Jack built.
%
%        This is the cat that scared the mouse that ate the malt hat lay in the house that Jack built.
%
%        This is the dog that chased the cat that scared the mouse that ate the malt that lay in the house that Jack built.
%
%        This is the boy who loves the dog that chased the cat that scared the mouse that ate the malt that lay in the house that Jack built.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\slide{Hierarchical Model}
%
%Comparison with phrase-based model:
%
%\begin{table}[ht]
%\begin{tabular}{| c c | c c |}
%\hline 
%		& 	& Phrase-based & Hierarchical \\
%\hline
%BLEU (Europarl) & fr-en & 25.10 & 24.58 \\
%		& de-en & 18.11 & 17.99 \\
%		& es-en & 25.81 & 25.17 \\
%		& de-en & 18.11 & 17.99 \\
%		& cs-en & 18.00 & 17.86 \\
%\hline
%Phrase-table size & fr-en & 2.5GB & 20.0GB \\
%\hline
%Decoding time (sec) & per sentence & 2.27 & 6.45 \\
%		    & per word & 0.09 & 0.26 \\
%\hline
%\end{tabular}
%
%\end{table}
%}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Syntax-based Model Training with Moses}
\vspace{20mm}
\begin{itemize}
\item Command line
\begin{center}
\begin{SaveVerbatim}{myverb} 
train-model.perl -ghkm ...
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\end{center}

\item Example rule from model
\begin{center}
\littlecode{\tiny [X][NP-SB] also wanted [X][ADJA] [X][NN] [X] ||| [X][NP-SB] wollten auch die [X][ADJA] [X][NN] [S-TOP] |||  ...}
\end{center}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Syntax-based Decoding with Moses}

\begin{itemize} \itemsep -1mm
\item Command line   
\vspace{-4mm}
\begin{center}
\begin{SaveVerbatim}{myverb}
moses_chart -f moses.ini -i in.txt > out.txt
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\end{center}
\vspace{-4mm}
(like hierarchical)

\item Advantage \vspace{-2mm}
\begin{itemize}
\item can use outside linguistic information 
\item promises to solve important problems in SMT, e.g.\ long-range reordering
\end{itemize}

\item Disadvantages \vspace{-2mm}
\begin{itemize}
%\item difficult to get right
\item training slow and difficult to get right
\item requires syntactic parse annotation
	\begin{itemize}
	\item syntactic parsers available only for some languages % need to be trained with costly, hand-annotated treebank
	\item not designed for machine translation
	\item unreliable
	\end{itemize}
\end{itemize}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \slide{Moses Tree Representation}
% \vspace{10mm}
% \begin{center}
% \Tree [.TOP [.NP [.NE Musharrafs ] [.ADJA letzter ] [.NN Akt ]  ]  [.PUNC  ? ]  ]  \\
% 
% \vspace{10mm}
% 
% \includegraphics[scale=1]{tree-xml.png} 
%  \end{center}

%
%Training and input data is embellished with parse information
%  - either the parse information is on the source side - tree-to-string
%  - on the target side - string-to-tree
%  - have parse information for both - tree-to-tree
%
%
%Advantages
%   - can use outside linguistic information
%Disadvantages
%   - difficult to get  right
%   - many still underperform other models
%   - parser information unreliable
%      - not for correct domain
%      - not available for many languages
%          - need to be trained with costly treebank
%      - not optmized or suited for machine translation
%
% do some slides on moses implementation

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \slide{How do I get started?}
% 
% \begin{itemize} \itemsep -1mm
% 
% \item{Use the {\bf Experiment Management System (EMS)} } 
%   \begin{itemize}
%   \item Automate entire process
%   \item Support for multi-core servers. And Sun Grid Engines (SGE)
%   \item Reuse existing alignments or models when running multiple, similar experiments
%   \item Example config files available in Moses
%   \item Disadvantage --- not all features are implemented
%   \end{itemize}
% 
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\slide{Advanced Features}
%\vspace{-5mm}
%\textcolor{darkgrey}{
%\small
%\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item \currenttopic{Experiment Management System}
%\item {Faster Training}
%\item {Faster Decoding}
%\item {Moses Server}
%\item {Data and domain adaptation}
%\item {Instructions to decoder}
%\item {Input formats}
%\item {Output formats}
%%\item {Translation models}
%\item {Incremental Training}
%\end{itemize}
%}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% TODO: one slide about tuning?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Experiment Management System}
\begin{itemize} \itemsep -1mm
\item EMS automates the entire pipeline
\item One configuration file for all settings: record of all experimental details
\item Scheduler of individual steps in pipeline
\begin{itemize}
\item automatically keeps track of dependencies
%\item runs on single machine, multi-core machine, GridEngine cluster
\item parallel execution 
\item crash detection
\item automatic re-use of prior results
\end{itemize}
\item Fast to use
\begin{itemize}
\item set up a new experiment in minutes
\item set up a variation of an experiment in seconds
\end{itemize}

\item Disadvantage: not all Moses features are integrated

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{How does it work?}
\vspace{20mm}
\begin{itemize}
\item Write a configuration file
(typically by adapting an existing file)
\vspace{5mm}
\item Test:
\vspace{-10mm}
\begin{center}
\littlecode{\normalsize experiment.perl -config config}
\end{center}
\vspace{5mm}
\item Execute:
\vspace{-10mm}
\begin{center}
\littlecode{\normalsize experiment.perl -config config -exec}
\end{center}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}
\begin{center} \vspace{-5mm}
\includegraphics[scale=1.41]{ems-agenda-composite.pdf}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Web Interface}
\begin{center}
\includegraphics[scale=1]{web-interface-experiments.png}\\[5mm]
List of experiments
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{List of Runs}
\begin{center}\vspace{-9mm}
\includegraphics[scale=1]{web-interface-runs}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Analysis: Basic Statistics}
\begin{center}
\includegraphics[scale=1]{analysis-stats.png}
\end{center}
\begin{itemize}
\item Basic statistics
\begin{itemize}
\item n-gram precision
\item evaluation metrics
\item coverage of the input in corpus and translation model
\item phrase segmentations used
\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Analysis: Unknown Words}
\begin{center}
grouped by count in test set\\[5mm]
\includegraphics[scale=1]{analysis-unknown.png}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Analysis: Output Annotation}
\vspace{30mm}
\begin{center}
\includegraphics[scale=1.5]{analysis-bleu.png}\\[20mm]
Color highlighting to indicate n-gram overlap with reference translation\\[5mm]
darker bleu = word is part of larger n-gram match
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Analysis: Input Annotation}
\vspace{10mm}
\begin{center}
\includegraphics[scale=1.5]{analysis-coverage.png}\\[20mm]
\end{center}
\begin{itemize}
\item For each word and phrase, color coding and stats on
\begin{itemize}
\item number of occurrences in training corpus
\item number of distinct translations in translation model
\item entropy of conditional translation probability distribution $\phi(e|f)$ (normalized)
\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Analysis: Bilingual Concordancer}
\vspace{-5mm}
\begin{center}
\includegraphics[scale=0.75]{biconcor1.png}\\[5mm]
\includegraphics[scale=0.75]{biconcor2.png}\\
translation of input phrase in training data context
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Analysis: Alignment}
\vspace{30mm}
\begin{center}
\includegraphics[scale=1.5]{analysis-alignment.png}\\[10mm]
Phrase alignment of the decoding process\\[5mm]
(red border, interactive)
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Analysis: Tree Alignment}
\vspace{15mm}
\begin{center}
\includegraphics[scale=1.2]{analysis-tree-alignment.png}\\[10mm]
Uses nested boxes to indicate tree structure\\[3mm]
(red border, yellow shaded spans in focus, interactive)\\[3mm]
for syntax model, non-terminals are also shown
\end{center}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}
\vspace{-2.5cm}
\begin{center}
\includegraphics[width=26cm]{ems-syntax-viz.png}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Analysis: Comparison of 2 Runs}
\begin{center}
\includegraphics[scale=1]{analysis-comparison.png}\\[10mm]
Different words are highlighted\\[3mm]
sortable by most improvement, deterioration
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{}
\vspace{50mm}
\begin{center}
\Huge \bf Hands-On Session
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\small
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item \currenttopic{Faster Training}
\item {Faster Decoding}
\item {Moses Server}
\item {Data and domain adaptation}
\item {Instructions to decoder}
\item {Input formats}
\item {Output formats}
%\item {Translation models}
\item {Incremental Training}
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item \currenttopic{Faster Training}
  \begin{itemize}
  \item Tokenization
  \item Tuning
  \item Alignment
  \item Phrase-Table Extraction
  \item Train language model
  \end{itemize}
\end{itemize}
\ldots
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Faster Training}

\begin{itemize} \itemsep 10mm
\vspace{10mm}
\item Run steps in parallel (that do not depend on each other)

\item {Multicore Parallelization}\\[4mm]
\begin{SaveVerbatim}{myverb} 
  .../train-model.perl -parallel
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}

\item EMS: \\[4mm]
\begin{SaveVerbatim}{myverb} 
  [TRAINING]
  parallel = yes
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}


\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
  \begin{itemize}
  \item \currenttopic{Tokenization}
  \item Tuning
  \item Alignment
  \item Phrase-Table Extraction
  \item Train language model
  \end{itemize}
\end{itemize}
\ldots
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Faster Training}

\begin{itemize} \itemsep 10mm

\item Multi-threaded tokenization

\item {Specify number of threads}\\[4mm]
\begin{SaveVerbatim}{myverb} 
 .../tokenizer.perl -threads NUM
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}

\item EMS: \\[4mm]
\begin{SaveVerbatim}{myverb} 
 input-tokenizer = "$moses-script-dir/tokenizer/tokenizer.perl 
		    -threads NUM " 
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
  
  
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
  \begin{itemize}
  \item {Tokenization}
  \item \currenttopic{Tuning}
  \item Alignment
  \item Phrase-Table Extraction
  \item Train language model
  \end{itemize}
\end{itemize}
\ldots
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Faster Training}
\vspace{10mm}
\begin{itemize} \itemsep 10mm
\item Multi-threaded tokenization

\item {Specify number of threads}\\[4mm]
\begin{SaveVerbatim}{myverb} 
  .../mert -threads NUM
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}

\item EMS:\\[4mm]
\begin{SaveVerbatim}{myverb} 
  tuning-settings = "-threads NUM" 
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}

\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
  \begin{itemize}
  \item {Tokenization}
  \item {Tuning}
  \item \currenttopic{Alignment}
  \item Phrase-Table Extraction
  \item Train language model
  \end{itemize}
\end{itemize}
\ldots
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Faster Training}

\begin{itemize} \itemsep -1mm

\item {Word Alignment}
\item {Multi-threaded}
  \begin{itemize}
  \item    Use MGIZA, not GIZA++
  \begin{center}
    \littlecode{.../train-model.perl -mgiza -mgiza-cpus NUM} 
  \end{center}      
  EMS: 
  \begin{center}
    \littlecode{training-options = " -mgiza -mgiza-cpus NUM " } 
  \end{center}      
  \end{itemize}

\item {On: memory-limited machines}
  \begin{itemize}
  \item snt2cooc program requires 6GB+ memory
  \item Reimplementation uses 10MB, but take longer to run
  \begin{center}
    \littlecode{.../train-model.perl -snt2cooc snt2cooc.pl} 
  \end{center}      
  EMS:
  \begin{center}
    \littlecode{training-options = "-snt2cooc snt2cooc.pl"}
  \end{center}      

  \end{itemize}
\end{itemize}
       

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
  \begin{itemize}
  \item {Tokenization}
  \item {Tuning}
  \item {Alignment}
  \item \currenttopic{Phrase-Table Extraction}
  \item Train language model
  \end{itemize}
\end{itemize}
\ldots
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Faster Training}
\vspace{30mm}
\begin{itemize} \itemsep -1mm

\item {Phrase-Table Extraction}
  \begin{itemize}
  \item Split training data into NUM equal parts
  \item Extract concurrently
  \end{itemize}
  \begin{center}
    \littlecode{.../train-model.perl -cores NUM}
  \end{center}      
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Faster Training}
\vspace{10mm}
\begin{itemize} \itemsep -1mm
\item {Sorting}
  \begin{itemize}
  \item Rely heavily on Unix 'sort' command
    \item may take 50\%+ of translation model build time 
  \item Need to optimize for
     \begin{itemize}
      \item speed
      \item disk usage
     \end{itemize}

  \item Dependent on
    \begin{itemize}
    \item      sort version
    \item      Unix version
    \item      available memory
    \end{itemize}
  \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Faster Training}
\vspace{-5mm}
  \begin{itemize} \itemsep -1mm 
  \item Plain sorted\\[2mm]
  \begin{SaveVerbatim}{myverb}
 sort < extract.txt > extract.sorted.txt
  \end{SaveVerbatim}
  \colorbox{gray}{\BUseVerbatim{myverb}}

  \item Optimized for large server\\[2mm]
  \begin{SaveVerbatim}{myverb}
 sort --buffer-size 10G --parallel 5
      --batch-size 253 --compress-program [gzip/pigz] ...
  \end{SaveVerbatim}
  \colorbox{gray}{\BUseVerbatim{myverb}}
    \begin{itemize}
    \item Use 10GB of RAM --- the more the better
    \item 5 CPUs --- the more the better
    \item merge\-sort at most 253 files
    \item compress intermediate files --- less disk i/o
    \end{itemize}

\item In Moses:\\[2mm]
    \begin{SaveVerbatim}{myverb}
 .../train-model.perl -sort-buffer-size 10G -sort-parallel 5 
      -sort-batch-size 253 -sort-compress pigz 
    \end{SaveVerbatim}
    \colorbox{gray}{\BUseVerbatim{myverb}}

  \end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
  \begin{itemize}
  \item {Tokenization}
  \item {Tuning}
  \item {Alignment}
  \item {Phrase-Table Extraction}
  \item \currenttopic{Train language model}
  \end{itemize}
\end{itemize}
\ldots
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{KENLM Training}
\vspace{5mm}
\begin{itemize}
\item Can train very large language models with limited RAM\\
(on disk streaming)\\[5mm]
\begin{SaveVerbatim}{myverb} 
lmplz -o [order] -S [memory] < text > text.lm
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}

\item \littlecode{-o order} = n-gram order
\item \littlecode{-S memory} = How much memory to use.
	      \begin{itemize}
		\item \littlecode{NUM\%} = percentage of physical memory \vspace{2mm}
		\item \littlecode{NUM[b/K/M/G/T]} = specified amount in bytes, kilo bytes, etc.
	      \end{itemize}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\small
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
\item \currenttopic{Faster Decoding}
\item {Moses Server}
\item {Data and domain adaptation}
\item {Instructions to decoder}
\item {Input formats}
\item {Output formats}
%\item {Translation models}
\item {Incremental Training}
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
\item \currenttopic{Faster Decoding}
  \begin{itemize}
  \item Multi-threading
  \item Speed vs. Memory
  \item Speed vs. Quality
  \end{itemize}
\end{itemize}
\ldots
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
\item {Faster Decoding}
  \begin{itemize}
  \item \currenttopic{Multi-threading}
  \item Speed vs. Memory
  \item Speed vs. Quality
  \end{itemize}
\end{itemize}
\ldots
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Faster Decoding}

\vspace{15mm}
\begin{itemize}
\item Multi-threaded decoding 
\begin{center}
\littlecode{.../moses --threads NUM}
\end{center}
\item Easy speed-up

\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
\item {Faster Decoding}
  \begin{itemize}
  \item {Multi-threading}
  \item \currenttopic{Speed vs. Memory}
  \item Speed vs. Quality
  \end{itemize}
\end{itemize}
\ldots
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\slide{Speed vs. Memory Use}
%\begin{center} 
%\includegraphics[scale=1.4]{less-memory.pdf}
%\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Speed vs. Memory Use}
\begin{tabular}{p{10cm}c}
\vspace{-11cm}
Typical Europarl file sizes:
\begin{itemize} \itemsep -1mm
  \item Language model \vspace{-3mm}
	\begin{itemize}
  	\item  170 MB (trigram)
	\item 412 MB (5-gram)
	\end{itemize}
  \item Phrase table \vspace{-3mm}
	\begin{itemize}
  	\item  11GB
	\end{itemize}
  \item Lexicalized reordering \vspace{-3mm}
	\begin{itemize}
  	\item  9.4GB
	\end{itemize}
   \item[$\rightarrow$] total = 20.8 GB
\end{itemize}
&
\includegraphics[scale=1.4]{less-memory-europarl.pdf}
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Speed vs. Memory Use}
\vspace{10mm}
\begin{tabular}{p{13cm}c}
\vspace{-7cm}
\begin{itemize}
\item Load into memory
	\begin{itemize}
	\item long load time
	\item large memory usage
	\item fast decoding
	\end{itemize}
\end{itemize}
& \includegraphics[scale=0.8]{less-memory-europarl.pdf} \\[1cm]
\vspace{-4.5cm}
\begin{itemize}
\item Load-on-demand
	\begin{itemize}
  	\item store indexed model on disk
	\item binary format
	\item minimal start-up time, memory usage
	\item slower decoding
	\end{itemize}
\end{itemize}
&  \includegraphics[scale=0.8]{less-memory-europarl-on-disk.pdf}
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Binary Phrase Tables}
Create:
\begin{center}
\begin{SaveVerbatim}{myverb} 
  ./CreateOnDiskPt 1 1 4 100 2 pt.txt out.folder
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\end{center}

Change ini file: \\[-6mm]
\begin{center}
\begin{SaveVerbatim}{myverb} 
[feature]
PhraseDictionaryBinary name=TranslationModel0 \
   table-limit=20 \ num-features=4 \
   path=/.../phrase-table
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Lexical Reordering Table}

Create:
\begin{center}
\begin{SaveVerbatim}{myverb} 
  export LC_ALL=C \ 
  processLexicalTable -in r-t.txt -out out.file
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\end{center}

Change ini file: \\[-6mm]
\begin{center}
automatically detected
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Compact Phrase Table}
\begin{itemize}
\item Memory-efficient data structure
\begin{itemize}
\item phrase table 6--7 times smaller than on-disk binary table
\item lexical reordering table 12--15 times smaller than on-disk binary table
\item Fastest phrase-table implementation
\end{itemize}
\item Create with
\begin{center}
\begin{SaveVerbatim}{myverb} 
processPhraseTableMin
processLexicalTableMin
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\end{center}
\item Specify with 
\begin{center}
\begin{SaveVerbatim}{myverb} 
PhraseDictionaryCompact
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\end{center}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{KENLM}
\vspace{10mm}
\begin{itemize}
\item Developed by Kenneth Heafield (CMU / Edinburgh / Stanford / Bloomberg)
\item Fastest and smallest language model implementation
\item Compile from LM trained with SRILM\\[5mm]
\begin{SaveVerbatim}{myverb} 
build_binary model.lm model.binlm
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\item Specify in decoder\\[5mm]
\begin{SaveVerbatim}{myverb} 
[feature]
KENLM name=LM0 factor=0 path=/.../model.binlm order=5
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Speed vs. Quality}
\vspace{5mm}
\begin{center} 
\includegraphics[scale=1.4]{quality-vs-speed.pdf}\vspace{-20mm}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Speed vs. Quality}
\begin{center} 
\includegraphics[scale=0.95]{decoding-step5.pdf}
\end{center}
\begin{itemize} \itemsep -1mm
\item Decoder search creates very large number of partial translations ("hypotheses")
\item Decoding time $\sim$ number of hypotheses created
\item Translation quality $\sim$ number of hypothesis created
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Hypothesis Stacks}
\begin{center} 
\includegraphics[scale=1]{hypothesis-stacks-fw.pdf}
\end{center}
\vspace{-2mm}
\begin{itemize} \itemsep -2mm
\item Phrase-based: One stack per number of input words covered
\item Number of hypothesis created = \\
sentence length $\times$ stack size $\times$ applicable translation options
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Pruning Parameters}
\begin{itemize}
\item Regular beam search
\begin{itemize}
\item \littlecode{--stack NUM} max. number of hypotheses contained in each stack
\item \littlecode{--ttable-limit NUM} max. num. of translation options per input phrase
\vspace{2mm}
\item search time roughly linear with respect to each number
\end{itemize}
\item Cube pruning\\
(fixed number of hypotheses are added to each stack)
\begin{itemize}
\item \littlecode{--search-algorithm 1} turns on cube pruning
\item \littlecode{--cube-pruning-pop-limit NUM} number of hypotheses added to each stack
\vspace{2mm}
\item search time roughly linear with respect to pop limit
\vspace{2mm}
\item note: stack size and translation table limit have little impact in speed
\end{itemize}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Syntax Hypothesis Stacks}
\begin{center} 
\includegraphics[scale=1.5]{chart-stacks.pdf}
\end{center}
\vspace{-5mm}
\begin{itemize} \itemsep -2mm
\item One stack per input word span
\item Number of hypothesis created = \\
sentence length$^2$ $\times$  number of hypotheses added to each stack\\
\littlecode{--cube-pruning-pop-limit NUM} number of hypotheses added to each stack
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
\item {Faster Decoding}
\item \currenttopic{Moses Server}
\item {Data and domain adaptation}
\item {Instructions to decoder}
\item {Input formats}
\item {Output formats}
%\item {Translation models}
\item {Incremental Training}
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Moses Server}
\begin{itemize} \itemsep -1mm
\item Moses command line:\\[3mm]
\begin{SaveVerbatim}{myverb} 
  .../moses -f [ini] < [input file] > [output file]
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
    \begin{itemize}
      \item Not practical for commercial use
    \end{itemize}


\item Moses Server:\\[3mm]
    \begin{SaveVerbatim}{myverb} 
 .../mosesserver -f [ini] --server-port [PORT] --server-log [LOG]
    \end{SaveVerbatim}
    \colorbox{gray}{\BUseVerbatim{myverb}}
    \begin{itemize}
      \item Accept HTTP input. XML SOAP format
    \end{itemize}

\item Client:
    \begin{itemize}
      \item Communicate via http
      \item Example clients in Java and Perl
      \item Write your own client
      \item Integrate into your own application
    \end{itemize}
    
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\normalsize
\vspace{-5mm}
\textcolor{darkgrey}{
\small
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
\item {Faster Decoding}
\item {Moses Server}
\item \currenttopic{Data and domain adaptation}
  \begin{itemize}\vspace{-4mm}
  \item Train everything together
  \item Secondary phrase table
\item Domain indicator features
\item Interpolated language models
%\item TM-MT integration
  \end{itemize}
\end{itemize}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Data}
\vspace{15mm}
\begin{itemize}
\item Parallel corpora $\rightarrow$ translation model
\begin{itemize}
\item sentence-aligned translated texts
\item translation memories are parallel corpora
\item dictionaries are parallel corpora
\end{itemize}
\item Monolingual corpora $\rightarrow$ language model
\begin{itemize}
\item text in the target language
\item billions of words easy to handle
\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Domain Adaptation}
\vspace{10mm}
\begin{itemize}
\item The more data, the better
\item The more in-domain data, the better\\
(even in-domain monolingual data very valuable)
%\item Multiple models 
%\begin{itemize}
%\item train a translation model for each domain corpus
%\item train a language model for each domain corpus
%\item use all, tune weights for each model
%\item alternative: interpolate language model
%\end{itemize}
\item Always tune towards target domain
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
\item {Faster Decoding}
\item {Moses Server}
\item {Data and domain adaptation}
  \begin{itemize}\vspace{-5mm}
  \item \currenttopic{Train everything together}
  \item Secondary phrase table
\item Domain indicator features
\item Interpolated language models
%\item TM-MT integration
  \end{itemize}
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Default: Train Everything Together}
\vspace{20mm}
\begin{itemize} \itemsep 10mm
 
\item Easy to implement
  \begin{itemize}
  \item Concatenate new data with existing data
  \item Retrain
  \end{itemize}
\item Disadvantages: 
  \begin{itemize}
  \item Slower training for large amount of data
  \item Cannot weight old and new data separately
  \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Default: Train Everything Together}
\vspace{-1mm}
Specification in EMS:
\begin{itemize} \itemsep 5mm
\item Phrase-table\\[4mm] \small
    \begin{SaveVerbatim}{myverb} 
      [CORPUS]
      [CORPUS:in-domain]
      raw-stem = ....    
      [CORPUS:background]
      raw-stem = ....
    \end{SaveVerbatim}
    \colorbox{gray}{\BUseVerbatim{myverb}}
\item LM\\[4mm] \small
   \begin{SaveVerbatim}{myverb} 
      [LM]
      [LM:in-domain]
      raw-corpus = ....
      [LM:background]
      raw-corpus = ....
    \end{SaveVerbatim}
    \colorbox{gray}{\BUseVerbatim{myverb}}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
\item {Faster Decoding}
\item {Moses Server}
\item {Data and domain adaptation}
  \begin{itemize}\vspace{-5mm}
  \item {Train everything together}
  \item \currenttopic{Secondary phrase table}
\item Domain indicator features
\item Interpolated language models
%\item TM-MT integration
  \end{itemize}
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\slide{TM-MT Integration}
%\begin{itemize}
%\item Input sentence: \vspace{-5mm}
%\begin{center}
%\example{The second paragraph of Article \highlightOrange{21} is deleted .}
%\end{center}
%\item Fuzzy match in translation memory: \vspace{-5mm}
%\begin{center}
%\example{The second paragraph of Article \highlight{5} is deleted .}\\
%=\\
%\example{\highlightGreen{{\`A} l' article} \highlight{5} \highlightGreen{, le texte du deuxi{\'e}me alin{\'e}a est supprim{\'e} .}}
%\end{center}
%\item[] \textcolor{darkgreen}{\bf Output word(s) taken from the target TM}  \vspace{-5mm}
%\item[] \textcolor{darkorange}{\bf Input word(s) that still need to be translated by SMT}
%\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\slide{TM-MT Integration}

%\begin{itemize} \itemsep -1mm
%\item Translation memory-style fuzzy match
%  \begin{itemize}
%  \item For hierarchical decoding
%  \item Create long translation rule 'templates'
%  \item Best for use with parallel corpus with lots of repetition
%  \end{itemize}

%\item Add TM and word alignment as a special phrase table
%  \begin{itemize}
%    \item Use in addition to normal phrase table
%  \end{itemize}
%  \begin{SaveVerbatim}{myverb} 
%   [ttable-file]
%   11 0 0 3 source-corpus;target-corpus;word-alignment 
%   2 0 0 3 phrase table
%   6 0 0 3 glue-rules
%  \end{SaveVerbatim}
%  \colorbox{gray}{\BUseVerbatim{myverb}}

%\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Secondary Phrase Table}
\begin{itemize} \itemsep -1mm
\item Train initial phrase table and LM on baseline data

\item Train secondary phrase table and LM new/in-domain data

\item Use both in Moses
  \begin{itemize}

  \item Secondary phrase table
  \begin{SaveVerbatim}{myverb} 
    [feature]
    PhraseDictionaryMemory path=.../path.1
    PhraseDictionaryMemory path=.../path.2
    
    [mapping]
    0 T 0
    1 T 1
  \end{SaveVerbatim}
  \colorbox{gray}{\BUseVerbatim{myverb}}

  \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Secondary Phrase Table}
\begin{itemize} \itemsep -1mm
  \item
  \begin{itemize}
  \item Secondary LM\\[4mm]
  \begin{SaveVerbatim}{myverb} 
    [feature]
    KENLM path=.../path.1
    KENLM path=.../path.2
  \end{SaveVerbatim}
  \colorbox{gray}{\BUseVerbatim{myverb}}

  \end{itemize}

  \item Can give different weights for primary and secondary tables
  \item Not integrated into the EMS
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Secondary Phrase Table}
\small
\begin{itemize} \itemsep -1mm
\item Terminology/Glossary database
  \begin{itemize}
    \item fixed translation
    \item per client, project, etc
  \end{itemize}
\item Primary phrase table
  \begin{itemize}
    \item backoff to 'normal' phrase-table if no glossary term

  \begin{SaveVerbatim}{myverb} 
    [feature]
    PhraseDictionaryMemory path=.../glossary
    PhraseDictionaryMemory path=.../normal.phrase.table
    
    [mapping]
    0 T 0
    1 T 1
    
    [decoding-graph-backoff]
    0
    1
  \end{SaveVerbatim}
  \colorbox{gray}{\BUseVerbatim{myverb}}

    \end{itemize}
\end{itemize}
\normalsize 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \slide{Incremental training}
% \begin{itemize}
% \item Retrain everything
% \item Secondary phrase table
% \item \currenttopic{Incremental GIZA++ and dynamic suffix arrays}
% \item TM-MT integration
% 
% \end{itemize}
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \slide{Incremental GIZA++ and Dynamic Suffix Arrays}
% 
% \begin{itemize}
% \item Don't extract phrase table
% \item Store entire parallel corpus in memory
% 	\\ Suffix Array
% \item Add new parallel data to suffix array
% \\
% \item Need word alignment
%   \\ Use customized version of GIZA++
%   \\ Reuse word-alignment model from primary parallel data
% 
% \item Bleeding edge. Not integrated into EMS
% \end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
\item {Faster Decoding}
\item {Moses Server}
\item {Data and domain adaptation}
  \begin{itemize} \vspace{-5mm}
  \item {Train everything together}
  \item {Secondary phrase table}
  \item \currenttopic{Domain indicator features}
\item Interpolated language models
%  \item TM-MT integration
  \end{itemize}
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Domain Indicator Features}
\vspace{15mm}
\begin{itemize} \itemsep 5mm
\item One translation model
\item Flag each phrase pair's origin
\begin{itemize}
\item indicator: binary flag if it occurs in specific domain
\item ratio: how often it occurs in specific domain relative to all
\item subset: similar to indicator, but if in multiple domains, marked with multiple-domain feature
\end{itemize}
\item In EMS:\\[4mm] \small
\begin{SaveVerbatim}{myverb}
  [TRAINING]
  domain-features = "indicator"
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
\item {Faster Decoding}
\item {Moses Server}
\item {Data and domain adaptation}
  \begin{itemize} \vspace{-5mm}
  \item {Train everything together}
  \item {Secondary phrase table}
  \item Domain indicator features
\item \currenttopic{Interpolated language models}
%  \item TM-MT integration
  \end{itemize}
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Interpolated Language Models}
\vspace{5mm}
\begin{itemize} \itemsep 5mm
\item Train one language model per corpus
\item Combine them by weighting each according to its importance
\begin{itemize}
\item weights obtained by optimizing perplexity\\ of resulting language model on tuning set\\
(not the same as machine translation quality) \vspace{2mm}
\item models are linearly combined
\end{itemize}
\item EMS provides a section {\tt [INTERPOLATED-LM]} that needs to be commented out
\item Alternative: use multiple language models\\
(disadvantage: larger process, slower)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\slide{Advanced Features}
%\vspace{-5mm}
%\textcolor{darkgrey}{
%\begin{itemize} \itemsep -1mm
%%\item {How do I get started?}
%%\item {Experiment Management System}
%\item {Faster Training}
%\item {Faster Decoding}
%\item {Moses Server}
%\item {Data and domain adaptation}
%  \begin{itemize} \vspace{-5mm}
%  \item {Train everything together}
%  \item {Secondary phrase table}
%  \item Domain indicator features
%\item Interpolated language models
%  \item \currenttopic{TM-MT integration}
%  \end{itemize}
%\end{itemize}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\slide{TM-MT Integration}
%\begin{itemize}
%\item Input sentence: \vspace{-5mm}
%\begin{center}
%\example{The second paragraph of Article \highlightOrange{21} is deleted .}
%\end{center}
%\item Fuzzy match in translation memory: \vspace{-5mm}
%\begin{center}
%\example{The second paragraph of Article \highlight{5} is deleted .}\\
%=\\
%\example{\highlightGreen{{\`A} l' article} \highlight{5} \highlightGreen{, le texte du deuxi{\'e}me alin{\'e}a est supprim{\'e} .}}
%\end{center}
%\item[] \textcolor{darkgreen}{\bf Output word(s) taken from the target TM}  \vspace{-5mm}
%\item[] \textcolor{darkorange}{\bf Input word(s) that still need to be translated by SMT}
%\end{itemize}


%\slide{TM-MT Integration}

%\begin{itemize} \itemsep -1mm
%\item Translation memory-style fuzzy match
%  \begin{itemize}
%  \item For hierarchical decoding
%  \item Create long translation rule 'templates'
%  \item Best for use with parallel corpus with lots of repetition
%  \end{itemize}

%\item Add TM and word alignment as a special phrase table
%  \begin{itemize}
%    \item Use in addition to normal phrase table
%  \end{itemize}
%  \begin{SaveVerbatim}{myverb} 
%   [feature]
%   PhraseDictionaryFuzzyMatch ...
%   PhraseDictionaryMemory path=phrase-table.gz ...
%   PhraseDictionaryMemory path=glue-rules ...
%  \end{SaveVerbatim}
%  \colorbox{gray}{\BUseVerbatim{myverb}}

%\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\small
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
\item {Faster Decoding}
\item {Moses Server}
\item {Data and domain adaptation}
\item \currenttopic{Instructions to decoder}
\item {Input formats}
\item {Output formats}
%\item {Translation models}
\item {Incremental Training}
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Transliteration}
\begin{itemize}
  \item Languages are written in different scripts 
  \begin{itemize}
    \item Russian, Bulgarian and Serbian - written in Cyrillic script
    \item Urdu, Farsi and Pashto - written in Arabic script
    \item Hindi, Marathi and Nepalese - written in Devanagri
  \end{itemize}
  \item Transliteration can be used to translate OOVs and Named Entities
  \item Problem: Transliteration corpus is not always available
  \item Naive Solution:
  \begin{itemize}
    \item Crawl training data from Wikipedia titles
    \item Build character-based transliteration model
    \item Replace OOV words with 1-best transliteration
  \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Transliteration}

\begin{itemize}
  \item 2 methods to integrate into MT
  \item Post-decoding method
  \begin{itemize}  
    \item Use language model to pick best transliteration
    \item Transliteration features
  \end{itemize}
  \item In-decoding method
  \begin{itemize}  
    \item Integrate transliteration inside decoder
    \item Words can be translated OR transliterated
  \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Transliteration}

\begin{itemize}
  \item EMS:
  
  \begin{SaveVerbatim}{myverb}
  [TRAINING]
  transliteration-module = "yes"
  \end{SaveVerbatim}
  \colorbox{gray}{\BUseVerbatim{myverb}}
  
  \item Post-processing method

  \begin{SaveVerbatim}{myverb} 
   post-decoding-transliteration = "yes"
  \end{SaveVerbatim}
  \colorbox{gray}{\BUseVerbatim{myverb}}
  
  \item In-decoding method

  \begin{SaveVerbatim}{myverb} 
   in-decoding-transliteration = "yes"
   transliteration-file = /list of words to be transliterated/
  \end{SaveVerbatim}
  \colorbox{gray}{\BUseVerbatim{myverb}}
 
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\small
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
\item {Faster Decoding}
\item {Moses Server}
\item {Data and domain adaptation}
\item {Instructions to decoder}
\item \currenttopic{Input formats}
\item {Output formats}
%\item {Translation models}
\item {Incremental Training}
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\small
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
\item {Faster Decoding}
\item {Moses Server}
\item {Data and domain adaptation}
\item {Instructions to decoder}
\item {Input formats}
\item \currenttopic{Output formats}
%\item {Translation models}
\item {Incremental Training}
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{N-Best List}

\begin{itemize}
\item Input \vspace{-5mm}
\begin{center}
\example{es gibt verschiedene andere meinungen .}
\end{center}

\item  Best Translation \vspace{-5mm}
\begin{center}
\example{there are various different opinions .}
\end{center}

\item  Next nine best translations \vspace{-5mm}
{\footnotesize \begin{center}
\example{
there are various other opinions . \\
there are different different opinions . \\
there are other different opinions . \\
we are various different opinions . \\
there are various other opinions of . \\
it is various different opinions . \\
there are different other opinions . \\
it is various other opinions . \\
it is a different opinions .}
\end{center}}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Uses of N-Best Lists}
\vspace{20mm}
\begin{itemize}
\item  Let the translator choose from possible translations
\item  Reranker
	\begin{itemize}
	\item add more knowledge sources
	\item can take global view
	\item coherency of whole sentence
	\item coherency of document
	\end{itemize}
\item  Used to tune component weights
\end{itemize}

%n-best translations
%
%1. Best translation for the each input sentence
%2. 10-best translation for each input sentence
%- in sorted orde of best first
%      - think that the decoder can get good translation
%      - but not confident that the decoder will do a good job of finding the best translation.
%      - give 10 translations for the user to decide
%      
%      - in general, can ask the decoder to return the n-best sentences
%      
%      - more often used to give to a post-processing step. 
%      - let another algorithm decide which 1 really is the best sentence. Based on other critieria not in the decoder
%          - document level information.
%          
%          - pronoun translation from chinese/vietnamese to english.
%          - dependent on context.
%             -  no word for 'me'. Could be translated as nephew, uncle, grandfather, friend, depending on who you're talking to
%             -  'you' could be translated as nephew, uncle, grandfather, friend.
%   - external tools which looks at the whole document might do a better job finding the most appropriate. 
%   - give it 100-best translation, let it decide
% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{N-Best Lists in Moses}
\vspace{5mm}
\begin{center}
Argument to command line\\[2mm]
\begin{SaveVerbatim}{myverb}
 ./moses -n-bestlist n-best.file.txt [distinct] 100
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}

\vspace{10mm}
Output\\[2mm]
{\footnotesize \begin{SaveVerbatim}{myverb}
0 ||| there are various different opinions .  ||| d: 0 lm: -21.6664 w: -6 ...  ||| -113.734
0 ||| there are various other opinions .  ||| d: 0 lm: -25.3276 w: -6 ... ||| -114.004
0 ||| there are different different opinions .  ||| d: 0 lm: -27.8429 w: -6 ...  ||| -117.738
0 ||| there are other different opinions .  ||| d: -4 lm: -25.1666 w: -6 ...  ||| -118.007
0 ||| we are various different opinions .  ||| d: 0 lm: -28.1533 w: -6 ...  ||| -118.142
0 ||| there are various other opinions of .  ||| d: 0 lm: -33.7616 w: -7 ...  ||| -118.153
0 ||| it is various different opinions .  ||| d: 0 lm: -29.8191 w: -6  ... ||| -118.222
0 ||| there are different other opinions .  ||| d: 0 lm: -30.426 w: -6 ...  ||| -118.236
0 ||| it is various other opinions .  ||| d: 0 lm: -32.6824 w: -6 ... ||| -118.395
0 ||| it is a different opinions .  ||| d: 0 lm: -20.1611 w: -6 ...  ||| -118.434

\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}}
\end{center}
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Search Graph}

\begin{itemize}
\item  Input \vspace{-10mm}
\begin{center}
\example{er geht ja nicht nach hause}
\end{center}

\item Return internal structure from the decoder \vspace{-5mm}
\begin{center}
\includegraphics[scale=1.2]{search-graph.png}
\end{center}

\item Encode millions of other possible translations\\
(every path through the graph = 1 translation)

\end{itemize}


%Alternative to getting n-best translation.
%Get back internal structure of the decoder.
%
%Directed graph 
%  - left most node is represents a hypothesis that has translated nothing
%  - right most nodes have translated all words
%  - a path from the left to the right is a translation of the input sentence
%  - best translation is 1 of these paths
%  
%Many such paths (millions)
%  - each path is a translation that the decoder has considered

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Uses of Search Graphs}
\begin{tabular}{p{9cm}c}
\vspace{-12cm}
\begin{itemize}
\item  Let the translator choose
	\begin{itemize}
	\item Individual words or phrases
	\item 'Suggest' next phrase
	\end{itemize}
\item  Reranker
\item  Used to tune component weights
	\begin{itemize}
	\item More difficult than with n-best list
	\end{itemize}

\end{itemize}

&

\includegraphics[scale=1]{lattice-caitra.png}

\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Search Graphs in Moses}
\vspace{5mm}
\begin{center}
Argument to command line\\[2mm]
\begin{SaveVerbatim}{myverb}
 ./moses -output-search-graph search-graph.file.txt 
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}
\end{center}
\vspace{5mm}

\begin{center}
Argument to command line\\[2mm]
{\small \begin{SaveVerbatim}{myverb}
0 hyp=0 stack=0 forward=36 fscore=-113.734
0 hyp=75 stack=1 back=0 score=-104.943 ... covered=5-5 out=.
0 hyp=72 stack=1 back=0 score=-8.846 ... covered=4-4 out=opinions
0 hyp=73 stack=1 back=0 score=-10.661 ... covered=4-4 out=opinions of
\end{SaveVerbatim}
\colorbox{gray}{\BUseVerbatim{myverb}}

\vspace{-10mm}
\begin{tabular}{p{15cm}}
\begin{itemize} \itemsep -2mm
\item hyp - hypothesis id
\item  stack - how many words have been translated
\item score - total weighted score
\item covered - which words were translated by this hypothesis
\item out - target phrase
\end{itemize}
\end{tabular}}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\slide{Advanced Features}
\vspace{-5mm}
\textcolor{darkgrey}{
\small
\begin{itemize} \itemsep -1mm
%\item {How do I get started?}
%\item {Experiment Management System}
\item {Faster Training}
\item {Faster Decoding}
\item {Moses Server}
\item {Data and domain adaptation}
\item {Instructions to decoder}
\item {Input formats}
\item {Output formats}
%\item \currenttopic{Translation models}
\item {Incremental Training}
\end{itemize}
}
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Acknowledgements}
\vspace{5mm}
\begin{table}[h]
\begin{center}
\begin{tabular}{ c  c  c } 

\includegraphics[scale=0.3]{univ-edinburgh.pdf}
&
\includegraphics[scale=0.07]{charles.png}
\\[1cm]
\includegraphics[scale=1]{fbk.png} 
&
\includegraphics[scale=0.6]{maryland.png}
\\[1cm]
\includegraphics[scale=1.5]{mit.png}
&
\includegraphics[scale=1]{aachen.png}
\end{tabular}
\end{center}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\slide{Moses Developers}
%\vspace{10mm}
%\begin{center}
%\footnotesize \rm
%\begin{tabular}{cccc} 
%Abhishek Arun & 
%Adam Lopez & 
%Ales Tamchyna & 
%Alex \\
%Amittai Axelrod &
%Ankit Srivastava &
%Anthony Rousseau &
%Benjamin Gottesman \\ 
%Barry Haddow &
%Ondrej Bojar &
%Chris Callison-Burch & 
%Christine Corbett \\
%Christian Hardmeier &
%Christian Federmann &
%Lane Schwartz &
%David Talbot \\
%Edmund Huber &
%Evan Herbst &
%Andreas Eisele &
%Eva Hasler \\
%Frederic Blain &
%Brooke Cowan &
%Grace M. Ngai&
%Kenneth Heafield \\
%Hieu Hoang &
%H. Leal Fontes &
%Holger Schwenk &
%Josh Schroeder \\
%Jean-Baptiste Fouet &
%Joern Wuebker &
%Jorge Civera &
%Konrad Rawlik \\
%Abby Levenberg &
%Alexandra Birch &
%Bo Fu &
%M.J.Bellino-Machado \\
%Mauro Cettolo &
%Marcello Federico &
%Michael Auli &
%John Joseph Morgan \\
%Mark Fishel &
%Gabriele Antonio Musillo &
%Miles Osborne &
%Nadi Tomeh \\
%Nicola Bertoldi &
%Oliver Wilson &
%Pascual Martinez &
%Philipp Koehn \\
%Phil Williams &
%Bruno Pouliquen &
%Raphael Payen &
%Chris Dyer \\
%Joao LuÃ­s Rosas &
%Rico Sennrich &
%Herve Saint-Amand &
%Felipe Sanchez Martinez \\
%Sara Stymne &
%Steven B. Parks &
%Steven Buraje Poggel &
%Andre Lynum \\
%Yizhao Ni &
%David Kolovratnak &
%Sergio Penkale &
%Stephan \\
%Suzy Howlett &
%Wade Shen &
%Yang Gao &
%Tsuyoshi Okita \\
%Alexander Fraser &
%Richard Zens
%
%\end{tabular}
%\end{center}


\begin{comment}
\end{comment}


\end{document}

